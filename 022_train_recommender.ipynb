{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MLP\n",
    "import torch_geometric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_method = 'graphsage'\n",
    "training_years = [2016]\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(method, training_year, year = None):\n",
    "    if year == None: \n",
    "        year = training_year \n",
    "    if method == 'specter':\n",
    "        emb = pd.read_json(f'specter/{year}/output.jsonl', lines = True).set_index('paper_id')\n",
    "        return emb['embedding']\n",
    "    elif method == 'tfidf':\n",
    "        X = scipy.sparse.load_npz(f'embeddings/tfidf_({training_year})_{year}.npz')\n",
    "        with open(f'embeddings/tfidf_({training_year})_{year}_index.json') as infile:\n",
    "            ids = json.load(infile)\n",
    "        return {k:X[ids[k]] for k in ids}\n",
    "    else: \n",
    "        with open(f'embeddings/{method}_({training_year})_{year}.json','r') as infile:\n",
    "            return json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(torch_geometric.nn.models.MLP):\n",
    "    def __init__(self, channels, dropout = 0):\n",
    "        super().__init__(channel_list=channels, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(super().forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_citations = {} \n",
    "for year in training_years: \n",
    "    with open(f'co_citations/{year}.json', 'r') as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    pairs = []\n",
    "    for q in tqdm(d): \n",
    "        for c in d[q]: \n",
    "            pairs.append({'q': q, 'c': c, 'n': d[q][c]})\n",
    "            \n",
    "    emb = load_embeddings(emb_method, year)\n",
    "    \n",
    "    co_cit_df = pd.DataFrame(pairs)\n",
    "    if emb_method == 'tfidf':\n",
    "        co_cit_df.loc[:,'emb'] = co_cit_df.apply(lambda x: scipy.sparse.hstack([emb[x['q']],emb[x['c']]]), axis=1)\n",
    "    else:\n",
    "        co_cit_df.loc[:,'emb'] = co_cit_df.apply(lambda x: emb[x['q']] + emb[x['c']], axis=1)\n",
    "    co_citations[year] = co_cit_df[['emb','n']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([co_citations[year] for year in training_years], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sample = train_df[train_df['n'] > -1]\n",
    "negative_sample = train_df[train_df['n'] == -1].sample(len(positive_sample))\n",
    "train_df = pd.concat([negative_sample,positive_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if emb_method == 'tfidf':\n",
    "    X = scipy.sparse.vstack(train_df['emb'])\n",
    "else:\n",
    "    X = np.vstack(train_df['emb'])\n",
    "y = train_df['n'].map(lambda x: 0 if x == -1 else 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1)\n",
    "X_train, X_val = torch.from_numpy(X_train).float(), torch.from_numpy(X_val).float()\n",
    "y_train, y_val = torch.from_numpy(y_train).float(), torch.from_numpy(y_val).float() \n",
    "\n",
    "# m_train = X_train.mean(0, keepdim=True)\n",
    "# s_train = X_train.std(0, unbiased=False, keepdim=True)\n",
    "# X_train -= m_train\n",
    "# X_train /= s_train\n",
    "# X_val -= m_train\n",
    "# X_val /= s_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X_train.shape[1]\n",
    "\n",
    "model = Net([dim, 64, 1], dropout=0.2)\n",
    "learning_rate = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def _train_batch(inputs, targets):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def _evaluate(inputs, targets):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        val_loss = criterion(outputs, targets)\n",
    "    return val_loss.item()\n",
    "\n",
    "loss_list = []\n",
    "best_loss = 10000\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "batch_size = len(X_train)\n",
    "num_epochs = 1000\n",
    "num_batches = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # print(f'Epoch: {epoch}')\n",
    "    losses = np.array([])\n",
    "    # with range(num_batches) as tq:\n",
    "    for batch in range(num_batches):\n",
    "        batch_start = batch * batch_size\n",
    "        batch_end = (batch + 1) * batch_size\n",
    "        inputs = X_train[batch_start:batch_end].clone()\n",
    "        targets = y_train[batch_start:batch_end].reshape(-1, 1)\n",
    "        loss = _train_batch(inputs, targets)\n",
    "        losses = np.append(losses, loss)\n",
    "            # tq.set_postfix({'loss':'{:.3f}'.format(losses.mean())})\n",
    "\n",
    "    val_loss = _evaluate(X_val, y_val.reshape(-1, 1))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch %d, loss: %.4f, val_loss: %.4f'\n",
    "              % (epoch, losses.mean(), val_loss))\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_params = model.state_dict()\n",
    "\n",
    "    loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str = '_'.join([str(y) for y in training_years])\n",
    "torch.save(best_params, f'params/recommenders/{emb_method}_{y_str}_update.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
